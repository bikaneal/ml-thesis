{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CAvn19iskEm2XmmmWoIfAIBhdG_UZR3a",
      "authorship_tag": "ABX9TyNrCDT2KsnyEnuIkZdEBAoX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bikaneal/ml-thesis/blob/main/request.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytRE7XJi9WSf",
        "outputId": "b58203ec-7ef2-476b-82fa-6a9774a39f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy3) (0.7.2)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.10/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U assemblyai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyfe-n0YUp96",
        "outputId": "5927038e-9916-491a-841c-ec79ba3ad917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: assemblyai in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: httpx>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (0.27.0)\n",
            "Requirement already satisfied: pydantic!=1.10.7,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (2.7.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (4.12.1)\n",
            "Requirement already satisfied: websockets>=11.0 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (12.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.19.0->assemblyai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (2.18.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.19.0->assemblyai) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start by making sure the `assemblyai` package is installed.\n",
        "# If not, you can install it by running the following command:\n",
        "# pip install -U assemblyai\n",
        "#\n",
        "# Note: Some macOS users may need to use `pip3` instead of `pip`.\n",
        "\n",
        "import assemblyai as aai\n",
        "\n",
        "# Replace with your API key\n",
        "aai.settings.api_key = \"a33634c5dea14943a8826558e6c994ef\"\n",
        "\n",
        "config = aai.TranscriptionConfig(language_code=\"ru\", speaker_labels=True)\n",
        "# You can also transcribe a local file by passing in a file path\n",
        "FILE_URL = open('/content/drive/MyDrive/Colab_Notebooks/make/gametest.mp3', 'rb').read()\n",
        "\n",
        "transcriber = aai.Transcriber()\n",
        "transcript = transcriber.transcribe(FILE_URL, config=config)\n",
        "\n",
        "with open('transcript.txt', 'w') as file:\n",
        "    for utterance in transcript.utterances:\n",
        "        file.write(f\"Speaker {utterance.speaker}: {utterance.text}n\")"
      ],
      "metadata": {
        "id": "PuPtu4YXUn2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from flask import Flask, request, jsonify\n",
        "import pymorphy3\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "model = Word2Vec.load('/content/drive/MyDrive/Colab_Notebooks/models/word2vec_class/word2vec-classification-50')\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/models/word2vec_class/classifier.pkl', 'rb') as file:\n",
        "    classifier = pickle.load(file)\n",
        "\n",
        "def preprocess(text, stop_words, punctuation_marks, morph):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    preprocessed_text = []\n",
        "    for token in tokens:\n",
        "        if token not in punctuation_marks:\n",
        "            lemma = morph.parse(token)[0].normal_form\n",
        "            if lemma not in stop_words:\n",
        "                preprocessed_text.append(lemma)\n",
        "    return preprocessed_text\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "stop_words = stopwords.words('russian')\n",
        "punctuation_marks = ['!', ',', '(', ')', ':', '-', '?', '.', '..', '...', '«', '»']\n",
        "\n",
        "def document_vector(word2vec_model, doc):\n",
        "    # Отфильтровываем слова, которых нет в модели\n",
        "    doc = [word for word in doc if word in word2vec_model.wv]\n",
        "    if len(doc) == 0:\n",
        "        return np.zeros(word2vec_model.vector_size, dtype=np.float32)\n",
        "\n",
        "    # Считаем среднее всех векторов слов документа\n",
        "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
        "\n",
        "def get_vector(model, tokenized_text):\n",
        "    return document_vector(model, tokenized_text)\n",
        "\n",
        "def process_text(text):\n",
        "    speakers = {}\n",
        "    lines = text.split('n')  # каждая новая строка считается репликой\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            continue  # Пропускаем пустые строки\n",
        "\n",
        "        try:\n",
        "            speaker, utterance = line.split(': ', 1)\n",
        "        except ValueError:  # если строка не содержит спикера\n",
        "            continue\n",
        "\n",
        "        speaker = speaker.strip()\n",
        "        utterance = utterance.strip()\n",
        "\n",
        "        if speaker not in speakers:\n",
        "            speakers[speaker] = []\n",
        "\n",
        "        # Предобработка реплики перед подачей в модель\n",
        "        preprocessed_utterance = preprocess(utterance, stop_words, punctuation_marks, morph)\n",
        "\n",
        "        # Получение вектора для отдельно взятой реплики\n",
        "        new_vector = get_vector(model, preprocessed_utterance)\n",
        "        new_vector = np.array([new_vector])\n",
        "\n",
        "        # Получение предсказания модели\n",
        "        prediction = classifier.predict(new_vector)\n",
        "\n",
        "        if prediction:\n",
        "            # Принадлежность реплики данному спикеру уже гарантирована, добавляем ее в список\n",
        "            speakers[speaker].append(utterance)\n",
        "\n",
        "    return speakers"
      ],
      "metadata": {
        "id": "VGhGghNI1UJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3886b20-dd35-4748-a0df-c0209b4dfd7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('/content/transcript.txt').read()\n",
        "data\n",
        "lines = data.split('n')\n",
        "for line in lines:\n",
        "      if not line.strip():\n",
        "          continue  # Пропускаем пустые строки\n",
        "\n",
        "      try:\n",
        "          speaker, utterance = line.split(': ', 1)\n",
        "      except ValueError:  # если строка не содержит спикера\n",
        "          continue\n",
        "\n",
        "      speaker = speaker.strip()\n",
        "      utterance = utterance.strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "iNwSkVyiw9IC",
        "outputId": "a95f9991-5017-459e-e5fe-00ea68701e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-27-56b74fc9aede>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-56b74fc9aede>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    for line in lines:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_text(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vppQIwNrtau6",
        "outputId": "a49d5d47-09c9-4148-bc24-3f5ed5ca46ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Speaker A': ['А вот тоже я отправила сообщение тут же. А, ну вы пишете 13, предлагаете, а что насчёт 1, 2, что вы думаете по этому поводу?',\n",
              "  'Что, мы тогда ставим 21 или другие варианты есть? Так, ну 2... Юля и Анна пишут 21. Галина? 29? Нет, подождите, у них максимум 20 ставка.',\n",
              "  'Но если они пойдут во банку, у них 20, а мы 21 поставим Ну.',\n",
              "  'Что команда 2 всегда ставит 20, 22, такие. Мне кажется, надо меньше 10 тоже, я согласна. Это хорошо работает, когда они жгут ресурсы, и потом мы с ними работаем. Мне кажется, или 2, или... Я, конечно, за 2, чтобы они осторожны. Но вторая в начале у них была 23 ставка.',\n",
              "  'Мне кажется, они пойдут в лобанк, а нужно столько, чтобы перекрыть их.',\n",
              "  'Давайте сорок пять тогда, да?',\n",
              "  'Ну тогда давайте единицу.'],\n",
              " 'Speaker B': ['Да, нужно точно Галина, вы поставите? Спасибо Супер Да, точно.',\n",
              "  'Мы... Поздравляю в одном шаге от победы. Так, я предлагаю поставить семь.',\n",
              "  'Прости, я тебя вообще не слышу.',\n",
              "  'Да, мы таким же составом. Что, мы снова ставим один?'],\n",
              " 'Speaker C': ['Я предлагаю начать с минимальных ставок для того, чтобы раскрутить своих оппонентов на более высокие ставки. Предлагаю сделать синие яички, чтобы у нас осталось больше денег, у нас будет больше потом ставок перейти.',\n",
              "  'Галина, поставите?',\n",
              "  'Так. Ну что, как и так, я предлагаю поставить Либо 13, либо 17, потому.',\n",
              "  'В прошлый раз вообще... Я заметила, что вот если начинать примерно там... Либо с мерянки вставки, либо начинать с тринадцати и потом ставить поменьше. Ну, то есть как бы... Либо начинать вот надо с какой-то там с пяти или с трёх. А потом десятые прибавляется. Либо вот наоборот. Сначала с десятых больше десяти ставить. Например, одиннадцать или тринадцать. А потом уменьшать инициум. То есть вот тут надо практику.',\n",
              "  'Я предлагаю ставить единицу и как бы оставить... Ну то есть ставьте единицу уже.'],\n",
              " 'Speaker D': [],\n",
              " 'Speaker E': ['Нам же не нужно 29.',\n",
              "  'Да, давайте три, чтобы больше ресурсов осталось.']}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# URL вашего Flask приложения\n",
        "url = 'http://ml-thesis-bikanel.amvera.io/analyze'\n",
        "\n",
        "# Открыть файл в бинарном режиме\n",
        "with open('transcript.txt', 'rb') as f:\n",
        "    file_content = f.read()\n",
        "\n",
        "# Создать запрос с multipart/form-data\n",
        "response = requests.post(url, files={'file': file_content})\n",
        "\n",
        "# Обработать ответ сервера\n",
        "decoded_response = response.json()\n",
        "\n",
        "# Преобразование decoded_response в строку формата JSON\n",
        "json_string = json.dumps(decoded_response, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Запись строки JSON в файл\n",
        "with open('result.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(json_string)\n",
        "\n",
        "print('Готово. Данные загрузились в файл result.txt')\n"
      ],
      "metadata": {
        "id": "oToSPQj5pCyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d63864e-1a70-4c04-b8c2-0ca711742b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Готово. Данные загрузились в файл result.txt\n"
          ]
        }
      ]
    }
  ]
}